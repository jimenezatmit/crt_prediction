{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b27a8b47",
   "metadata": {},
   "source": [
    "# This file includes all the helper functions used in data_processing.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d718b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa65d74c",
   "metadata": {},
   "source": [
    "# Section 2: Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c2e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create regex expressions for different elements one would find in a tweet\n",
    "regex_str = [\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)+' # anything else\n",
    "]\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function preprocesses a text and deals with urls, mentions, and hashtags\n",
    "\n",
    "Inputs:\n",
    "s (string): text to preprocess\n",
    "lowercase (boolean): if True then convert string to all lowercase \n",
    "\n",
    "Outputs:\n",
    "preprocessed (string): preprocessed string\n",
    "\"\"\"\n",
    "\n",
    "def preprocess(s, lowercase=True):\n",
    "    \n",
    "    if type(s) != str and math.isnan(s):\n",
    "        return np.nan\n",
    "    \n",
    "    tokens = tokenize(s)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    html_regex = re.compile('<[^>]+>')\n",
    "    tokens = [token for token in tokens if not html_regex.match(token)]\n",
    "\n",
    "    mention_regex = re.compile('(?:@[\\w_]+)')\n",
    "    tokens = ['@user' if mention_regex.match(token) else token for token in tokens]\n",
    "\n",
    "    url_regex = re.compile('http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+')\n",
    "    tokens = ['!url' if url_regex.match(token) else token for token in tokens]\n",
    "\n",
    "    hashtag_regex = re.compile(\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\")\n",
    "    tokens = ['' if hashtag_regex.match(token) else token for token in tokens]\n",
    "\n",
    "    flag = False\n",
    "    for item in tokens:\n",
    "        if item=='rt':\n",
    "            flag = True\n",
    "            continue\n",
    "        if flag and item=='@user':\n",
    "            return ''\n",
    "        else:\n",
    "            flag = False\n",
    "    \n",
    "    preprocessed = ' '.join([t for t in tokens if t]).replace('rt @user : ','')\n",
    "\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function tokenizes a single string\n",
    "\n",
    "Inputs:\n",
    "s (string): text to tokenize\n",
    "\n",
    "Outputs: \n",
    "tokenized (list): all tokens of the given string given regex expressions above\n",
    "\"\"\"\n",
    "def tokenize(s):\n",
    "    tokenized = tokens_re.findall(s)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1aa351",
   "metadata": {},
   "source": [
    "# Section 3: Exclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8202dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function applies relevant exclusions to the input dataframe\n",
    "\n",
    "Inputs:\n",
    "df (pandas df): input dataframe\n",
    "\n",
    "Outputs:\n",
    "df (pandas df): dataframe with a\n",
    "\"\"\"\n",
    "\n",
    "def apply_exclusions(df):\n",
    "    \n",
    "    # drop NaN and empty CRT score \n",
    "    for c in df.columns:\n",
    "        if c.startswith('CRT'):    \n",
    "            df = df[df.c.notna()]\n",
    "            df = df[df.c != ' ']\n",
    "\n",
    "    # drop participants with NaN or empty screen name \n",
    "    df = df[df['screen_name'].notna()]\n",
    "    df = df[df.screen_name!=' ']\n",
    "    \n",
    "    # drop participants with age < 18\n",
    "    df = df[df['age']>=18] \n",
    "    \n",
    "    # drop participants with followers >= 7,000 followers\n",
    "    df = df[df['followers_count']<7000] \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa7e5580",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function checks if the order of text in df_1 is the same as that of df_2\n",
    "\n",
    "Inputs:\n",
    "df_1 (pandas df): dataframe from t2v_single.txt\n",
    "d2_2 (pandas df): dataframe from t2v_mappings.txt\n",
    "\n",
    "Outputs:\n",
    "is_ordered (boolean): if True, text columns from df_1 and df_2 are identical \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def is_ordered(df_1, df_2):\n",
    "    is_ordered = df_1['text'].equals(df_2['text'])\n",
    "    return is_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f377c4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d58daec",
   "metadata": {},
   "source": [
    "# This script transforms the raw data (numerical, text) into numerical feature representations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d89a522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "# general packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import sys\n",
    "import copy\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import time\n",
    "import os\n",
    "\n",
    "# pipeline packages\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap.umap_ as umap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc832a",
   "metadata": {},
   "source": [
    "# Build Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "662b5755",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function builds a transformer for numerical features \n",
    "\n",
    "Inputs:\n",
    "data (pandas df): master dataframe \n",
    "feat (string): single target feature name\n",
    "log (boolean): if True, log transform the feature\n",
    "\n",
    "Outputs:\n",
    "to_transform (list): List of a transformers. Each transformer has the form\n",
    "('transformer_name', Pipeline, ['feature_name'])\n",
    "\"\"\"\n",
    "\n",
    "def build_num_transformer(data, feat, log):\n",
    "    \n",
    "    to_transform = []\n",
    "    \n",
    "    constant = abs(data[feat].min()) + 0.001\n",
    "        \n",
    "    if log:\n",
    "        to_transform.append(('log', Pipeline([('log', FunctionTransformer(lambda x: np.log(x + constant))), \n",
    "                                             ('scale', StandardScaler())]), [feat]))\n",
    "    else:\n",
    "        to_transform.append(('none', Pipeline([('scale', StandardScaler())]), [feat]))\n",
    "    \n",
    "    return to_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03cf9ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function builds a transformer for text features (domains, hashtags, mentions, followees)\n",
    "\n",
    "Inputs:\n",
    "feat (string): single target feature name\n",
    "my_components (integer): best number of components for TruncatedSVD from tuning\n",
    "my_min_df (integer): best min_df for TF-IDF from tuning\n",
    "\n",
    "Outputs:\n",
    "to_transform (list): List of a transformers. Each transformer has the form\n",
    "('transformer_name', Pipeline, ['feature_name'])\n",
    "\"\"\"\n",
    "def build_text_transformer(feat, my_components, my_min_df):\n",
    "    \n",
    "    to_transform = []\n",
    "    \n",
    "    text_pipes = {\n",
    "\n",
    "        'hashtags': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(ngram_range=(1,1),max_df= 1.0,min_df = my_min_df,use_idf=True,binary=False)),\n",
    "            ('svd', TruncatedSVD(n_components=my_components, random_state=17))\n",
    "        ]),\n",
    "        \n",
    "        'domains': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(ngram_range=(1,1),max_df= 1.0,min_df = my_min_df,use_idf=True,binary=False)),\n",
    "            ('svd', TruncatedSVD(n_components=my_components, random_state=17))\n",
    "        ]),\n",
    "        \n",
    "        'followees': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(ngram_range=(1,1),max_df= 1.0,min_df = my_min_df,use_idf=True,binary=True)),\n",
    "            ('svd', TruncatedSVD(n_components=my_components, random_state=17))\n",
    "        ]),\n",
    "\n",
    "        'mentions': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(ngram_range=(1,1),max_df= 1.0,min_df = my_min_df,use_idf=True,binary=False)),\n",
    "            ('svd', TruncatedSVD(n_components=my_components, random_state=17))\n",
    "        ]),\n",
    "        \n",
    "        'bio': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(ngram_range=(1,1), max_df= 1.0, min_df=my_min_df, use_idf=True,binary=False)),\n",
    "            ('svd', TruncatedSVD(n_components=my_components, random_state=17))\n",
    "        ]),\n",
    "        \n",
    "        'follower_bios': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(ngram_range=(1,1), max_df= 1.0, min_df=my_min_df, use_idf=True,binary=False)),\n",
    "            ('svd', TruncatedSVD(n_components=my_components, random_state=17))\n",
    "        ]),\n",
    "        \n",
    "        'followee_bios': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(ngram_range=(1,1), max_df= 1.0, min_df=my_min_df, use_idf=True,binary=False)),\n",
    "            ('svd', TruncatedSVD(n_components=my_components, random_state=17))\n",
    "        ]),\n",
    "        \n",
    "        'text': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(ngram_range=(1,1), max_df= 1.0, min_df=my_min_df, use_idf=True,binary=False)),\n",
    "            ('svd', TruncatedSVD(n_components=my_components, random_state=17))\n",
    "        ])\n",
    "        \n",
    "    }\n",
    "\n",
    "    to_transform.append((feat, text_pipes[feat], feat))\n",
    "    \n",
    "    return to_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a4bb05",
   "metadata": {},
   "source": [
    "# Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcd1af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function returns a list of feature names\n",
    "\n",
    "Inputs:\n",
    "data (pandas df): master dataframe \n",
    "\n",
    "Outputs:\n",
    "feats (list): list of all features from dataframe\n",
    "\"\"\"\n",
    "def get_features(data):\n",
    "    feats = list(data.columns)\n",
    "    feats.remove('screen_name')\n",
    "    feats.remove('age')\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be968894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function creates master dataframe using pipeline builders and T2V dataframe. \n",
    "\n",
    "Inputs:\n",
    "data (pandas df): master dataframe \n",
    "target (string): CRT target feature (numeric, conceptual, or both)\n",
    "best_components (dictionary of integers): best n_components for TruncatedSVD from tuning\n",
    "best_mindf (dictionary of lists): best min_df for TF-IDF from tuning\n",
    "results_path (string): path to store output df\n",
    "\n",
    "Outputs:\n",
    "df_i (pandas df): transformed master dataframe \n",
    "\"\"\"\n",
    "def create_dataframe(data, target, best_components, best_mindf, results_path, data_type, k_dict={}, l_dict={}):\n",
    "    \n",
    "    df_i = pd.DataFrame(data['screen_name'])\n",
    "    \n",
    "    features = get_features(data)\n",
    "\n",
    "    for f_name in features:\n",
    "            \n",
    "        if f_name.startswith('CRT'):\n",
    "            continue\n",
    "        \n",
    "        # transform data that is not null \n",
    "        data_select = data[['screen_name', f_name]]\n",
    "        data_in = data_select[data_select[f_name].notnull()] \n",
    "        data_nan = data_select[~data_select[f_name].notnull()] \n",
    "\n",
    "        # build transformers\n",
    "        if data[f_name].dtype in [float, int, np.float64, np.int64]:\n",
    "            skew = data[f_name].skew(axis=0, skipna=True) # check if high or medium skew \n",
    "            if -0.5 > skew or 0.5 < skew:\n",
    "                log = True\n",
    "            else:\n",
    "                log = False\n",
    "            trans = build_num_transformer(data, f_name, log)\n",
    "            preprocessor = ColumnTransformer(trans, remainder='passthrough')\n",
    "            output = preprocessor.fit_transform(pd.DataFrame(data_in))\n",
    "        else:\n",
    "            output = None\n",
    "            if data_type == 'test': \n",
    "                k = 0\n",
    "            else:\n",
    "                k = k_dict[f_name]\n",
    "            \n",
    "            while output is None: \n",
    "                if data_type == 'test':\n",
    "                    k = k + 1\n",
    "                try:                     \n",
    "                    trans = build_text_transformer(f_name, best_components[f_name][0], best_mindf[f_name][k])\n",
    "                    preprocessor = ColumnTransformer(trans, remainder='passthrough')\n",
    "                    output = preprocessor.fit_transform(pd.DataFrame(data_in))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if data_type == 'test':\n",
    "                l = 0\n",
    "            else:\n",
    "                l = l_dict[f_name]\n",
    "            \n",
    "            while output.shape[1] != (best_components[f_name][l] + 1):\n",
    "                if data_type == 'test':\n",
    "                    l = l + 1\n",
    "                try:                     \n",
    "                    trans = build_text_transformer(f_name, best_components[f_name][l], best_mindf[f_name][k])\n",
    "                    preprocessor = ColumnTransformer(trans, remainder='passthrough')\n",
    "                    output = preprocessor.fit_transform(pd.DataFrame(data_in))\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            k_dict[f_name] = k\n",
    "            l_dict[f_name] = l\n",
    "        \n",
    "        cols = []\n",
    "        \n",
    "        if data[f_name].dtype in [float, int, np.float64, np.int64]:\n",
    "            cols.append(f_name)\n",
    "        else:\n",
    "            for i in range(len(output[0]) - 1):\n",
    "                cols.append(f_name + \"_\" + str(i))\n",
    "        cols.append('screen_name')\n",
    "                \n",
    "        df_temp = pd.DataFrame(output, columns = cols).set_index('screen_name')\n",
    "        \n",
    "        for user in data_nan.screen_name:\n",
    "            if data_type.lower() in ['full', 'dropped']:\n",
    "                df_temp.loc[user] = np.nan\n",
    "            elif data_type.lower() == 'imputed':\n",
    "                df_temp.loc[user] = df_temp.mean()\n",
    "\n",
    "        df_i = pd.merge(left=df_i, right=df_temp.reset_index(), on='screen_name', how='right')\n",
    "    \n",
    "    # add CRT score back in\n",
    "    target_cols = [i for i in list(data.columns) if i.startswith('CRT')]\n",
    "    target_cols.append('screen_name')\n",
    "    \n",
    "    df_i = pd.merge(left=df_i, right=data[target_cols], on='screen_name', how='right')\n",
    "    \n",
    "    # round CRT score for bucket purposes\n",
    "    for i in df_i.columns:\n",
    "        if i.startswith('CRT'):\n",
    "            df_i[i] = df_i[i].apply(lambda x: round(x, 4))\n",
    "    \n",
    "    isExist = os.path.exists(results_path)\n",
    "    if not isExist:\n",
    "        os.makedirs(results_path)\n",
    "        \n",
    "    df_i.to_csv(results_path + \"{}.csv\".format('data_transformed_' + data_type))\n",
    "    \n",
    "    return df_i, k_dict, l_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "454a0026",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function merges the old and new data into one master dataframe \n",
    "\n",
    "Inputs: \n",
    "old_data_link (string): path to old data \n",
    "new_data_transformed (pandas df): dataframe containing all new users\n",
    "results_folder (string): path to folder name\n",
    "\n",
    "Outputs: \n",
    "master_df (pandas df): combined df\n",
    "\"\"\"\n",
    "def merge_dataframes(old_data_link, new_data_transformed, results_folder, data_type):\n",
    "\n",
    "    if old_data_link == None:\n",
    "        master_df = new_data_transformed\n",
    "    else:\n",
    "        old_data_transformed = pd.read_csv(old_data_link, index_col=0)\n",
    "        master_df = pd.concat([old_data_transformed, new_data_transformed], join=\"inner\", ignore_index=True)\n",
    "    \n",
    "    # Check whether the specified path exists or not\n",
    "    isExist = os.path.exists(results_folder)\n",
    "    if not isExist:\n",
    "        os.makedirs(results_folder)\n",
    "    \n",
    "    master_df.to_csv(results_folder + \"{}.csv\".format('master_data_' + data_type))\n",
    "    \n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b51a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
